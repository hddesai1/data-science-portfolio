{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "357989df-bd87-408a-99fb-e165117136f2",
   "metadata": {
    "id": "357989df-bd87-408a-99fb-e165117136f2"
   },
   "source": [
    "## **Install & import relevant packages**\n",
    "\n",
    "To build a simple chatbot using Python, we need to install the following packages. Each package serves a specific purpose in our chatbot pipeline:\n",
    "\n",
    "- **`ollama`**: This package allows us to run large language models (LLMs) locally. It simplifies interactions with models like LLaMA and Mistral.\n",
    "- **`langchain`**: A framework for building applications powered by LLMs. It provides tools for chaining prompts, managing memory, and integrating models.\n",
    "- **`chromadb`**: A vector database used for storing and retrieving text embeddings. This is essential for making the chatbot context-aware.\n",
    "- **`gradio`**: A simple way to build web-based interfaces for machine learning models. We’ll use it to create a user-friendly chatbot interface.\n",
    "- **`langchain-community`**: A collection of integrations and utilities that extend `langchain`, making it easier to work with external tools and databases.\n",
    "- **`pymupdf`**: To work with PDF documents, we need to install `pymupdf` which makes it easy to handle PDF files."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q git+https://github.com/HawkClaws/oyama.git ollama"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PV4uGeQgNxMg",
    "outputId": "4e9b1520-dbd7-4378-ee17-690c61ec1365"
   },
   "id": "PV4uGeQgNxMg",
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for oyama (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b56b22d-880c-4dc1-8325-3aea0f31511b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "collapsed": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7b56b22d-880c-4dc1-8325-3aea0f31511b",
    "outputId": "dbf025e0-9f70-4e30-fdf5-0dd955a07b2c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.21)\n",
      "Collecting chromadb\n",
      "  Downloading chromadb-0.6.3-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting gradio\n",
      "  Downloading gradio-5.23.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.47)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.7)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.18)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.39)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
      "  Downloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
      "Collecting fastapi>=0.95.2 (from chromadb)\n",
      "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.0.2)\n",
      "Collecting posthog>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-3.21.0-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.12.2)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.31.1)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.31.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.52b1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.31.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.1)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
      "Collecting overrides>=7.3.1 (from chromadb)\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.71.0)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.2)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.0.0)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.15)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting gradio-client==1.8.0 (from gradio)\n",
      "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting groovy~=0.1 (from gradio)\n",
      "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.29.3)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
      "Collecting pydub (from gradio)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.18 (from gradio)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting ruff>=0.9.3 (from gradio)\n",
      "  Downloading ruff-0.11.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
      "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.0)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (1.33)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.69.2)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.31.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.31.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-proto==1.31.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.31.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.52b1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-instrumentation==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation-0.52b1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.52b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n",
      "Collecting opentelemetry-util-http==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_util_http-0.52b1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Downloading chromadb-0.6.3-py3-none-any.whl (611 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gradio-5.23.1-py3-none-any.whl (51.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
      "Downloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.31.1-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.31.1-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.31.1-py3-none-any.whl (55 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.52b1-py3-none-any.whl (12 kB)\n",
      "Downloading opentelemetry_instrumentation-0.52b1-py3-none-any.whl (31 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.52b1-py3-none-any.whl (16 kB)\n",
      "Downloading opentelemetry_util_http-0.52b1-py3-none-any.whl (7.3 kB)\n",
      "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Downloading posthog-3.21.0-py2.py3-none-any.whl (79 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading ruff-0.11.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
      "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
      "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
      "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53800 sha256=cf947b33ccce7cde503da65b33b959d70712df7b24b6fa7d20768fe4a7f3a5de\n",
      "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, pydub, monotonic, durationpy, uvloop, uvicorn, tomlkit, semantic-version, ruff, python-multipart, python-dotenv, pyproject_hooks, overrides, opentelemetry-util-http, opentelemetry-proto, mmh3, humanfriendly, httptools, groovy, ffmpy, chroma-hnswlib, bcrypt, backoff, asgiref, aiofiles, watchfiles, starlette, posthog, opentelemetry-exporter-otlp-proto-common, coloredlogs, build, safehttpx, onnxruntime, kubernetes, gradio-client, fastapi, opentelemetry-instrumentation, gradio, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
      "Successfully installed aiofiles-23.2.1 asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.6.3 coloredlogs-15.0.1 durationpy-0.9 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.23.1 gradio-client-1.8.0 groovy-0.1.2 httptools-0.6.4 humanfriendly-10.0 kubernetes-32.0.1 mmh3-5.1.0 monotonic-1.6 onnxruntime-1.21.0 opentelemetry-exporter-otlp-proto-common-1.31.1 opentelemetry-exporter-otlp-proto-grpc-1.31.1 opentelemetry-instrumentation-0.52b1 opentelemetry-instrumentation-asgi-0.52b1 opentelemetry-instrumentation-fastapi-0.52b1 opentelemetry-proto-1.31.1 opentelemetry-util-http-0.52b1 overrides-7.7.0 posthog-3.21.0 pydub-0.25.1 pypika-0.48.9 pyproject_hooks-1.2.0 python-dotenv-1.1.0 python-multipart-0.0.20 ruff-0.11.2 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.1 tomlkit-0.13.2 uvicorn-0.34.0 uvloop-0.21.0 watchfiles-1.0.4\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.20-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.47)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.21 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.21)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.39)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.14)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.18)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.21->langchain-community) (0.3.7)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.21->langchain-community) (2.10.6)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (2.27.2)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
      "Downloading langchain_community-0.3.20-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
      "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.20 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.8.1 typing-inspect-0.9.0\n",
      "Collecting pymupdf\n",
      "  Downloading pymupdf-1.25.4-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
      "Downloading pymupdf-1.25.4-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.25.4\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain chromadb gradio\n",
    "!pip install -U langchain-community\n",
    "!pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09504989-e4da-443c-98c0-db67fc15f22f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "id": "09504989-e4da-443c-98c0-db67fc15f22f"
   },
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "from oyama import oyama\n",
    "import ollama # Enables interaction with local large language models (LLMs)\n",
    "import gradio as gr  # Provides an easy-to-use web interface for the chatbot\n",
    "\n",
    "# Document processing and retrieval\n",
    "from langchain_community.document_loaders import PyMuPDFLoader  # Extracts text from PDF files for processing\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Splits text into smaller chunks for better embedding and retrieval\n",
    "from langchain.vectorstores import Chroma  # Handles storage and retrieval of vector embeddings using ChromaDB\n",
    "\n",
    "# Embedding generation\n",
    "from langchain_community.embeddings import OllamaEmbeddings  # Converts text into numerical vectors using Ollama's embedding classifier\n",
    "\n",
    "import re  # Provides tools for working with regular expressions, useful for text cleaning and pattern matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa13b5d5-15d3-463d-b794-6980a1f329f7",
   "metadata": {
    "id": "aa13b5d5-15d3-463d-b794-6980a1f329f7"
   },
   "source": [
    "## **Call DeepSeek R1 1.5B via API**\n",
    "\n",
    "In this snippet, we use `ollama.chat()` to generate a response from DeepSeek R1 1.5B (which is installed locally). Let’s break it down:\n",
    "\n",
    "- **Choosing the Model**: We specify `\"deepseek-r1:1.5b\"` using the `model` argument.\n",
    "- **Passing User Messages**: The `messages` parameter is a list of interactions, where each message contains:\n",
    "  - `\"role\": \"user\"` – Indicates that the message is from the user.\n",
    "  - `\"content\": \"Explain Newton's second law of motion\"` – The actual question asked.\n",
    "- **Extracting and Printing the Response**: The model generates a structured response, where the content of the reply is stored in `response[\"message\"][\"content\"]`. We print this output to display the answer.\n",
    "\n",
    "This approach allows us to interact with an LLM locally, making it a powerful way to answer queries without relying on external APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea826cef-9dba-4441-bf66-f50098955fe1",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ea826cef-9dba-4441-bf66-f50098955fe1",
    "outputId": "3ad7c183-86b2-4736-bd1f-a75465fd0d44"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "command:=ollama --version\n",
      "Output: Warning: could not connect to a running Ollama instance\n",
      "Warning: client version is 0.6.2\n",
      "Warning: could not connect to a running Ollama instance\n",
      "Warning: client version is 0.6.2\n",
      "command:=ollama serve\n",
      "Server is not ready yet. Retrying...\n",
      "Server is ready.\n",
      "command:=ollama pull deepseek-r1:1.5b\n",
      "Output: \u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling aabd4debf0c8... 100% ▕████████████████▏ 1.1 GB                         \u001b[K\n",
      "pulling 369ca498f347... 100% ▕████████████████▏  387 B                         \u001b[K\n",
      "pulling 6e4c38e1172f... 100% ▕████████████████▏ 1.1 KB                         \u001b[K\n",
      "pulling f4d24e9138dd... 100% ▕████████████████▏  148 B                         \u001b[K\n",
      "pulling a85fe2a2e58e... 100% ▕████████████████▏  487 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling aabd4debf0c8... 100% ▕████████████████▏ 1.1 GB                         \u001b[K\n",
      "pulling 369ca498f347... 100% ▕████████████████▏  387 B                         \u001b[K\n",
      "pulling 6e4c38e1172f... 100% ▕████████████████▏ 1.1 KB                         \u001b[K\n",
      "pulling f4d24e9138dd... 100% ▕████████████████▏  148 B                         \u001b[K\n",
      "pulling a85fe2a2e58e... 100% ▕████████████████▏  487 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n",
      "Enable Model:deepseek-r1:1.5b\n"
     ]
    }
   ],
   "source": [
    "model_path = \"deepseek-r1:1.5b\"\n",
    "model_name = oyama.run(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Call the Ollama model to generate a response\n",
    "response = ollama.chat(\n",
    "    classifier=model_name,  # Specifies the DeepSeek R1 classifier (1.5B parameters)\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain Newton's second law of motion\"},  # User's input query\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Print the chatbot's response\n",
    "print(response[\"message\"][\"content\"])  # Extracts and displays the generated response from the classifier"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a49FFo_tOoQI",
    "outputId": "f07f623e-59f4-4109-d9b9-bc3d58824594"
   },
   "id": "a49FFo_tOoQI",
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<think>\n",
      "Okay, so I need to explain Newton's Second Law of Motion. Hmm, I remember it has something to do with force and acceleration. Let me think about what I know. \n",
      "\n",
      "Newton's laws are all about how objects move when forces act on them. The first one is about if there's no net force, the object doesn't accelerate—so that makes sense because if you're just sitting there, nothing moves unless something pushes or pulls.\n",
      "\n",
      "The second law seems more important because it's a fundamental principle. I think it says that the acceleration of an object is directly proportional to the net force acting on it and inversely proportional to its mass. So, F equals mass times acceleration, right? But how do I explain this?\n",
      "\n",
      "Maybe I should use a simple example. Like when you push a chair. If someone pushes it harder, it accelerates faster. So more force means higher acceleration. On the other hand, if there's a heavier chair, the same push would cause less acceleration because of its mass.\n",
      "\n",
      "Wait, but how do I translate that into an equation? Newton's second law is F = ma, where m is mass and a is acceleration. But why does this work? Maybe it's about the relationship between force and change in velocity over time. So if you apply more force, over the same time period, the object speeds up more.\n",
      "\n",
      "I should also consider other examples to make it clearer. Like when you accelerate a car—more force required for higher acceleration because of the mass of the car. Or when something gets faster—it must be experiencing a net force acting on it.\n",
      "\n",
      "What about units? Force is measured in Newtons, mass in kilograms, and acceleration in meters per second squared. So F = 6.1 kg * a m/s² gives N. That makes sense because 1 N is 1 kg·m/s².\n",
      "\n",
      "I should also mention that this law works when the forces are balanced, but it's about net force. If multiple forces are acting on an object, you add them up to get the net force and then calculate acceleration from there.\n",
      "\n",
      "Maybe I can explain it in terms of cause and effect—force causes acceleration because mass is how much inertia something has. So more inertia (higher mass) means less change in speed for a given force.\n",
      "\n",
      "Wait, what if someone tries to stop an object? If they push back with the same force, but it's on the opposite direction, that changes its velocity, hence acceleration. Newton's law explains why it takes longer to stop something heavy compared to moving a light one.\n",
      "\n",
      "I think I've got a good grasp now. Let me organize this into a clear explanation.\n",
      "</think>\n",
      "\n",
      "Newton's Second Law of Motion states that the acceleration of an object is directly proportional to the net force acting on it and inversely proportional to its mass. Mathematically, this relationship is expressed as F = ma, where F is the net force in Newtons (N), m is the mass of the object in kilograms (kg), and a is the acceleration in meters per second squared (m/s²). \n",
      "\n",
      "To illustrate this law, consider pushing an object: when you exert a greater force on it, its acceleration increases. Conversely, for a more massive object, the same applied force results in less change in velocity over time. This law explains how different forces and masses affect motion, showing that net force is the primary factor determining acceleration.\n",
      "\n",
      "The law also applies to scenarios like accelerating vehicles—more mass requires more force to achieve higher acceleration. In dynamic situations, such as applying force to stop an object, Newton's Second Law helps predict the resulting change in velocity due to the opposing force exerted by the environment. This principle underscores the importance of understanding forces and their effects on motion in various physical scenarios.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c69a03-fee0-4198-8d55-e68a0f650ad6",
   "metadata": {
    "id": "85c69a03-fee0-4198-8d55-e68a0f650ad6"
   },
   "source": [
    "## Preprocess the PDF Document for RAG\n",
    "\n",
    "We will now create a function that pre-processes the PDF file for RAG. Below is a breakdown of its logic:\n",
    "\n",
    "- **Check if a PDF is provided**: If no file is uploaded, the function returns `None`, preventing unnecessary processing.\n",
    "- **Extract text from the PDF**: Uses `PyMuPDFLoader` to load and extract raw text from the document.\n",
    "- **Split the text into chunks**: Since LLMs process smaller text fragments better, we use `RecursiveCharacterTextSplitter`. Each chunk contains **500 characters**, with an **overlap of 100 characters** to maintain context.\n",
    "- **Generate embeddings for each chunk**: Uses `OllamaEmbeddings` with the `\"deepseek-r1:1.5b\"` model to convert text into **numerical vectors**. These embeddings allow us to find **meaning-based matches** rather than exact keyword searches.\n",
    "- **Store embeddings in a vector database**: We use `ChromaDB` to **store and organize** the generated embeddings efficiently. The data is **persisted** in `\"./chroma_db\"` to avoid recomputing embeddings every time.\n",
    "- **Create a retriever for searching the database**: The retriever acts like a **smart search engine**, enabling the chatbot to fetch the most relevant text when answering questions.\n",
    "- **Return essential components**\n",
    "    - `text_splitter` (for future text processing)\n",
    "    - `vectorstore` (holding the document embeddings)\n",
    "    - `retriever` (allowing AI-powered search over the document)\n",
    "\n",
    "## **What are embeddings?**\n",
    "Embeddings are **numerical representations of text** that capture meaning. Instead of treating words as just sequences of letters, embeddings transform them into **multi-dimensional vectors** where similar words or sentences **are placed closer together**.\n",
    "\n",
    "![image](https://miro.medium.com/v2/resize:fit:1400/1*OEmWDt4eztOcm5pr2QbxfA.png)\n",
    "_Source: https://medium.com/towards-data-science/word-embeddings-intuition-behind-the-vector-representation-of-the-words-7e4eb2410bba_\n",
    "\n",
    "### **Intuition: how do embeddings work?**\n",
    "Imagine a **map of words**:\n",
    "- Words with **similar meanings** (*cat* and *dog*) are **closer together**.\n",
    "- Words with **different meanings** (*cat* and *car*) are **farther apart**.\n",
    "- Sentences or paragraphs with similar **context** will have embeddings that are **close to each other**.\n",
    "\n",
    "This means when a user asks a question, the LLM doesn’t just look for **exact words**—it finds the **most relevant text based on meaning**, even if the wording is different.\n",
    "\n",
    "### **Why this matters?**\n",
    "This function enables a chatbot to **understand and retrieve information from PDFs efficiently**. Instead of simple keyword searches, it **finds contextually relevant information**, making AI responses **more accurate and useful**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6d2b0f7-a3ec-418f-9807-20b6ddba252d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "id": "c6d2b0f7-a3ec-418f-9807-20b6ddba252d"
   },
   "outputs": [],
   "source": [
    "# Define the function that processes the PDF\n",
    "def process_pdf(pdf_bytes):\n",
    "    # If PDF files are empty, return None — This prevents errors from trying to process an empty input.\n",
    "    if pdf_bytes is None:\n",
    "        return None, None, None\n",
    "    # PyMuPDFLoader initializes the PDF file\n",
    "    loader = PyMuPDFLoader(pdf_bytes)\n",
    "    # .load() method reads the content of the PDF and extracts its text\n",
    "    dataset = loader.load()\n",
    "    # RecursiveCharacterTextSplitter splits the PDF into chunks of 500 characters, keeping 100 characters overlap to keep context\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    # Splits the documents into chunks and stores them in chunks object\n",
    "    chunks = text_splitter.split_documents(dataset)\n",
    "    # Create embeddings using OllamaEmbeddings\n",
    "    embeddings = OllamaEmbeddings(classifier=\"deepseek-r1:1.5b\")\n",
    "    # Create a vector database which allows us to store the chunks and their embeddings\n",
    "    vectorstore=Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=\"./chroma_db\")  # Example directory\n",
    "    # This creates a retriever that enables searching through the vectorstore.\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    \"\"\"\n",
    "    The function returns 3 objects\n",
    "        text_splitter → (Used to split new text in the same way as before)\n",
    "        vectorstore → (Holds the processed document chunks)\n",
    "        retriever → (Used to fetch relevant document chunks when answering questions)\n",
    "    \"\"\"\n",
    "\n",
    "    return text_splitter, vectorstore, retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ec4a86-8335-4f52-bbee-2a2c8085fcaa",
   "metadata": {
    "id": "76ec4a86-8335-4f52-bbee-2a2c8085fcaa"
   },
   "source": [
    "## **Combining retrieved document chunks**\n",
    "Once the embeddings are retrieved, next we need to stitch these together. The `combine_docs() function merges multiple retrieved document chunks into a single string. Why do we do this?\n",
    "\n",
    "- **Provides better context** – LLMs understand structured, continuous text better than fragmented pieces.  \n",
    "- **Improves response quality** – Merging chunks helps LLMs generate more coherent and complete answers.  \n",
    "- **Preserves document flow** – Keeps information logically ordered, preventing disjointed responses.  \n",
    "- **Optimizes token usage** – Reduces redundant queries and ensures efficient use of the model’s context window.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdce7c97-8c46-402f-a574-42139ad67509",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "id": "fdce7c97-8c46-402f-a574-42139ad67509"
   },
   "outputs": [],
   "source": [
    "def combine_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33982b85-946a-42e5-80f5-3369ffb9df1e",
   "metadata": {
    "id": "33982b85-946a-42e5-80f5-3369ffb9df1e"
   },
   "source": [
    "## Querying DeepSeek-R1 using Ollama\n",
    "\n",
    "Now, our input to the model is ready. Let’s set up DeepSeek R1 using Ollama.\n",
    "\n",
    "The `ollama_llm()` function **takes a user’s question and relevant context, formats a structured prompt, sends it to the DeepSeek-R1 model, and returns a clean generated response**.\n",
    "\n",
    "### **How it works (step-by-step)**\n",
    "- **Formats the input** – Structures the question and context for better input understanding.\n",
    "- **Calls `deepseek-r1`** – Sends the formatted prompt to generate a response.\n",
    "- **Extracts the response content** – Retrieves the AI’s answer.\n",
    "- **Cleans unnecessary text** – Removes `<think>...</think>` traces that contain model reasoning.\n",
    "- **Returns the cleaned response** – Provides a polished and readable AI answer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f5d31ca-42ba-4c0e-8f7b-5233e776dc98",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "id": "6f5d31ca-42ba-4c0e-8f7b-5233e776dc98"
   },
   "outputs": [],
   "source": [
    "def ollama_llm(question, context):\n",
    "\n",
    "    # Format the prompt with the question and context to provide structured input for the AI\n",
    "    formatted_prompt = f\"Question: {question}\\n\\nContext: {context}\"\n",
    "    # Send the structured prompt to the Ollama model for processing\n",
    "    response = ollama.chat(\n",
    "        classifier=\"deepseek-r1:1.5b\",  # Specifies the AI classifier to use\n",
    "        messages=[{'role': 'user', 'content': formatted_prompt}]  # Formats the user input\n",
    "    )\n",
    "    # Extract the AI-generated response content\n",
    "    response_content = response['message']['content']\n",
    "    # Remove content inside <think>...</think> tags to clean up AI reasoning traces\n",
    "    final_answer = re.sub(r'<think>.*?</think>', # We're searching for think tags\n",
    "                          '', # We'll replace them with empty spaces\n",
    "                          response_content, # In response_content\n",
    "                          flags=re.DOTALL).strip() # (dot) should match newlines (\\n) as well.\n",
    "    # Return the final cleaned response\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106285be-8517-4744-bae6-81722e6e0d5c",
   "metadata": {
    "id": "106285be-8517-4744-bae6-81722e6e0d5c"
   },
   "source": [
    "## **Build a RAG pipeline**\n",
    "\n",
    "Now we have all the required components, let’s build the RAG pipeline for our demo. We will build the `rag_chain()` function, which **retrieves relevant document chunks, formats them, and generates a response with the additional context from the retrieval step**.\n",
    "\n",
    "### **How it works**\n",
    "\n",
    "- **Retrieves relevant document chunks**: The `retriever.invoke(question)` searches for the most relevant text based on the user's question.Instead of relying solely on a language model’s memory, it **fetches factual data** from stored documents.\n",
    "- **Formats the retrieved content**: `combine_docs(retrieved_docs)` merges the document chunks into a single structured text. This ensures that DeepSeek receives a **well-organized input** for better reasoning.\n",
    "- **Generates the response**: Calls `ollama_llm(question, formatted_content)`, which:  \n",
    "    - Passes the structured input to `deepseek-r1:1.5b` for processing.  \n",
    "    - Cleans up the response (removes `<think>` tags).  \n",
    "    - Returns a polished, fact-based answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecf97fc9-c2a1-45c0-b716-56c5a9d30708",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "id": "ecf97fc9-c2a1-45c0-b716-56c5a9d30708"
   },
   "outputs": [],
   "source": [
    "# Define rag_chain function for Retrieval Augmented Generation\n",
    "def rag_chain(question, text_splitter, vectorstore, retriever):\n",
    "    \"\"\"\n",
    "    This function takes as input:\n",
    "        - The question we want to ask the classifier\n",
    "        - The text_splitter object to split the PDF and read into chunks\n",
    "        - The vectorstore for retrieving embeddings\n",
    "        - The retriever objects which retrieves dataset from the vectorstore\n",
    "    \"\"\"\n",
    "    retrieved_docs = retriever.invoke(question) # In this step, we will find the part of the document most relevant to the question\n",
    "    formatted_content = combine_docs(retrieved_docs) # We will then combine the retrieved parts of the document\n",
    "    return ollama_llm(question, formatted_content) # Run the classifier on the question, and the relevant context from the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28e6a285-8920-4717-a33d-972c3e54e19d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "id": "28e6a285-8920-4717-a33d-972c3e54e19d"
   },
   "outputs": [],
   "source": [
    "# Put it all together — Create a function that performs the logic expected by the Chatbot\n",
    "def ask_question(pdf_bytes, question):\n",
    "    text_splitter, vectorstore, retriever = process_pdf(pdf_bytes) # Process the PDF\n",
    "    if text_splitter is None:\n",
    "        return None  # No PDF uploaded\n",
    "    result = rag_chain(question, text_splitter, vectorstore, retriever) # Return the results with RAG\n",
    "    return {result}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9e8fcf-1d5f-4c5c-9bc2-9a14550eee1c",
   "metadata": {
    "id": "1a9e8fcf-1d5f-4c5c-9bc2-9a14550eee1c"
   },
   "source": [
    "## **Building a Chat Interface with Gradio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbbdfcc2-466b-4190-8711-b9ebce57b1b3",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 614
    },
    "id": "bbbdfcc2-466b-4190-8711-b9ebce57b1b3",
    "outputId": "4ee384b3-cac0-42e1-fb40-115dfa81fe60"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://b295778c490d6c30e7.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div><iframe src=\"https://b295778c490d6c30e7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# Define a Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=ask_question,  # The function that processes user input and generates a response (logic of the app)\n",
    "    inputs=[\n",
    "        gr.File(label=\"Upload PDF (optional)\"),  # Optional file upload input for a PDF document\n",
    "        gr.Textbox(label=\"Ask a question\")  # Text input where the user types their question\n",
    "    ],\n",
    "    outputs=\"text\",  # The function returns a text response\n",
    "    title=\"Ask questions about your PDF\",  # The title displayed on the interface\n",
    "    description=\"Use DeepSeek-R1 1.5B to answer your questions about the uploaded PDF document.\",  # Brief description of the interface's functionality\n",
    ")\n",
    "\n",
    "# Launch the Gradio interface to start the web-based app\n",
    "interface.launch(share = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71f0953-6a97-469a-825c-22161f386acb",
   "metadata": {
    "id": "e71f0953-6a97-469a-825c-22161f386acb"
   },
   "source": [
    "## **How to uninstall Ollama and DeepSeek?**\n",
    "\n",
    "**To uninstall Ollama**\n",
    "\n",
    "To uninstall Ollama\n",
    "\n",
    "- macOS: Delete Ollama from applications\n",
    "- Windows: Uninstall Ollama using control panel\n",
    "\n",
    "**To uninstall DeepSeek**\n",
    "\n",
    "Search for these folders on your computer — delete these folders.\n",
    "\n",
    "- macOS: ~/.ollama/models\n",
    "- Windows: C:\\Users\\%username%\\.ollama\\models\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}