# Day 39. 100 Days of Data Science Challenge - 03/11/2025

## ğŸš€ MLflow Experiment Tracking â€“ [DagsHub Repository](https://dagshub.com/hddesai1/mario_vs_wario)

## ğŸŒŸ Project Overview  

Machine learning models donâ€™t exist in isolationâ€”they evolve through **constant experimentation, hyperparameter tuning, and iterative improvements**. But how do you keep track of all these experiments?  

Enter **MLflow**â€”an open-source platform that brings order to chaos by enabling **live experiment tracking, model registry, and seamless deployment**.  

In this project, I integrated **MLflow with DagsHub**, setting up a **fully remote tracking server** to log parameters, metrics, and artifactsâ€”ensuring complete reproducibility and collaboration in ML projects.  

ğŸš€ **Key Highlights:**  
âœ”ï¸ Set up a **remote MLflow Tracking Server** with DagsHub  
âœ”ï¸ Implemented **live logging of ML experiments**  
âœ”ï¸ Tracked **hyperparameters, performance metrics, and artifacts**  
âœ”ï¸ Automated **model versioning and experiment comparison**  

---

## ğŸ¯ Why Does This Matter?  

ğŸ“Œ **Reproducibility is a Nightmare Without Experiment Tracking**  
- Have you ever lost track of **which model version had the best accuracy**?  
- Have you ever struggled to **replicate an experiment after months**?  

**MLflow solves this!** It provides an easy way to:  
âœ… **Log every run** â€“ No more guessing what parameters worked best  
âœ… **Compare experiments** â€“ Quickly identify improvements over past models  
âœ… **Deploy seamlessly** â€“ Register models for production use  

This project takes **machine learning from â€œjust codeâ€ to a structured, scalable, and trackable process**.  

---

## ğŸ“‚ Data Collection & Setup  

ğŸ”¹ **Dataset:** Trained a deep learning model for **image classification (Mario vs. Wario Dataset)**  
ğŸ”¹ **Experimentation Environment:** Google Colab & DagsHub  
ğŸ”¹ **Tracking Setup:** Used **MLflow with remote logging**  

---

## ğŸ›  Analytical Approach  

### 1ï¸âƒ£ **Experiment Tracking with MLflow**  
- Logged **hyperparameters**, **model metrics**, and **artifacts** for every experiment  
- Stored results **remotely** on DagsHub, enabling **team collaboration**  

### 2ï¸âƒ£ **Automated Model Versioning**  
- Each **trained model was automatically versioned** in MLflow  
- Tracked **which experiment performed best** for future deployment  

### 3ï¸âƒ£ **Hyperparameter Tuning**  
- Tracked how **learning rate, batch size, and epochs** influenced performance  
- Compared runs to find **optimal configurations**  

### 4ï¸âƒ£ **Seamless Deployment Integration**  
- **MLflow Model Registry** stored the best-performing model  
- Enabled **quick deployment to AWS** or local servers  

---

## ğŸ”¥ Key Insights & Findings  

| **Experiment ID**  | **Accuracy** | **Loss** | **Hyperparameters**            |  
|--------------------|-------------|----------|--------------------------------|  
| `exp_001`         | 67.3%       | 0.64     | LR = 0.001, Epochs = 5         |  
| `exp_002`         | 58.2%       | 0.69     | LR = 0.0005, Epochs = 10       |  
| `exp_003`         | 72.8%       | 0.59     | LR = 0.0008, Epochs = 7        |  

### âœ¨ Observations  

- **Experiment 003 performed best** with **72.8% accuracy**  
- Increasing **epochs beyond 7 led to overfitting**  
- Learning rate **0.0008 provided optimal convergence**  

---

## ğŸ¨ MLflow Dashboard Highlights  

âœ… **Interactive UI for Experiment Tracking** â€“ View and compare past runs  
âœ… **Live Metric Logging** â€“ Track loss, accuracy, and validation scores in real-time  
âœ… **Model Registry & Versioning** â€“ Store, retrieve, and deploy the best models  

ğŸš€ **[View Experiment Logs](https://dagshub.com/hddesai1/mario_vs_wario/experiments)**  

---

## ğŸš§ Challenges & Solutions  

### Challenge: **Tracking ML Experiments Across Multiple Runs**  
âœ… **Solution:** Used **MLflow Tracking Server** to automatically log and retrieve experiment history  

### Challenge: **Choosing the Best Model Version**  
âœ… **Solution:** Implemented **automated model comparison** using MLflowâ€™s experiment dashboard  

### Challenge: **Remote Storage for Artifacts**  
âœ… **Solution:** Used **DagsHub integration** to store artifacts and logs in a **centralized cloud repository**  

---

## ğŸ’¡ Future Enhancements  

ğŸ”¹ **Hyperparameter Optimization** â€“ Use **Optuna or Bayesian Optimization** to find the best parameters automatically  
ğŸ”¹ **Real-time Monitoring** â€“ Set up **Grafana dashboards** for live performance tracking  
ğŸ”¹ **Deployment Pipeline** â€“ Automate model deployment using **CI/CD with MLflow Models**  

---


### âœ¨ Final Thoughts  

This project bridges the gap between **machine learning and MLOps**, making experiments **traceable, reproducible, and scalable**. **MLflow is not just a toolâ€”itâ€™s a necessity for every serious data scientist.**  
ğŸ’¬ **Letâ€™s discuss!** If youâ€™re passionate about **ML experiment tracking, automation, or MLOps**, letâ€™s connect and exchange ideas! ğŸ˜Š  
